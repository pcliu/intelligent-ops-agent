# DeepSeek LLM é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨æ™ºèƒ½è¿ç»´æ™ºèƒ½ä½“ä¸­é…ç½®å’Œä½¿ç”¨ DeepSeek å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸŒŸ ä¸ºä»€ä¹ˆé€‰æ‹© DeepSeek

### ä¼˜åŠ¿
- **é«˜æ€§ä»·æ¯”**: API è°ƒç”¨æˆæœ¬è¿œä½äº GPT-4
- **ä¸­æ–‡ä¼˜åŒ–**: å¯¹ä¸­æ–‡ç†è§£å’Œç”Ÿæˆæ•ˆæœä¼˜ç§€
- **ä»£ç èƒ½åŠ›**: DeepSeek-Coder åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°çªå‡º
- **å¼€æºé€æ˜**: æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•å…¬å¼€
- **API å…¼å®¹**: æ”¯æŒ OpenAI æ ¼å¼çš„ API è°ƒç”¨

### é€‚ç”¨åœºæ™¯
- è¿ç»´æ—¥å¿—åˆ†æï¼ˆä¸­æ–‡æ—¥å¿—å‹å¥½ï¼‰
- æ•…éšœè¯Šæ–­æŠ¥å‘Šç”Ÿæˆ
- è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆ
- è¿ç»´çŸ¥è¯†é—®ç­”
- æˆæœ¬æ•æ„Ÿçš„å¤§è§„æ¨¡éƒ¨ç½²

## ğŸš€ å¿«é€Ÿé…ç½®

### 1. è·å– API Key
1. è®¿é—® [DeepSeek å®˜ç½‘](https://www.deepseek.com/)
2. æ³¨å†Œè´¦æˆ·å¹¶è·å– API Key
3. å……å€¼è´¦æˆ·ï¼ˆæ”¯æŒå¾®ä¿¡ã€æ”¯ä»˜å®ï¼‰

### 2. ç¯å¢ƒé…ç½®
åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# å¤åˆ¶ç¤ºä¾‹é…ç½®æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim .env
```

è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼š
```bash
# DeepSeek é…ç½®
DEEPSEEK_API_KEY="your-deepseek-api-key-here"
LLM_PROVIDER="deepseek"
LLM_MODEL_NAME="deepseek-chat"
LLM_BASE_URL="https://api.deepseek.com/v1"
LLM_TEMPERATURE="0.1"
LLM_MAX_TOKENS="2000"
```

### 3. æµ‹è¯•è¿æ¥
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œ DeepSeek æµ‹è¯•
python examples/deepseek_test.py
```

## ğŸ“‹ é…ç½®é€‰é¡¹

### æ”¯æŒçš„æ¨¡å‹
| æ¨¡å‹åç§° | æè¿° | é€‚ç”¨åœºæ™¯ | å»ºè®®å‚æ•° |
|---------|------|----------|----------|
| `deepseek-chat` | é€šç”¨å¯¹è¯æ¨¡å‹ | å‘Šè­¦åˆ†æã€è¯Šæ–­æŠ¥å‘Š | temp=0.1, max_tokens=2000 |
| `deepseek-coder` | ä»£ç ä¸“ç”¨æ¨¡å‹ | è„šæœ¬ç”Ÿæˆã€ä»£ç åˆ†æ | temp=0.1, max_tokens=4000 |

### å‚æ•°è°ƒä¼˜å»ºè®®
```python
# ä¿å®ˆé…ç½®ï¼ˆé«˜å‡†ç¡®æ€§ï¼‰
LLMConfig(
    provider="deepseek",
    model_name="deepseek-chat",
    temperature=0.1,        # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
    max_tokens=2000,        # é€‚ä¸­çš„è¾“å‡ºé•¿åº¦
    timeout=60              # åˆç†çš„è¶…æ—¶æ—¶é—´
)

# åˆ›æ„é…ç½®ï¼ˆæ›´å¤šæ ·åŒ–è¾“å‡ºï¼‰
LLMConfig(
    provider="deepseek",
    model_name="deepseek-chat",
    temperature=0.3,        # ç¨é«˜æ¸©åº¦å¢åŠ åˆ›æ„
    max_tokens=4000,        # æ›´é•¿çš„è¾“å‡º
    timeout=120             # æ›´é•¿çš„è¶…æ—¶æ—¶é—´
)
```

## ğŸ”§ ä»£ç é›†æˆ

### åŸºç¡€ä½¿ç”¨
```python
from src.utils.llm_config import setup_deepseek_llm, LLMConfig

# é…ç½® DeepSeek
config = LLMConfig(
    provider="deepseek",
    model_name="deepseek-chat",
    api_key="your-api-key"
)

# åˆå§‹åŒ– LLM
dspy_lm, langchain_llm = setup_deepseek_llm(config)

# DSPy ä½¿ç”¨
response = dspy_lm("è¯·åˆ†æè¿™ä¸ªå‘Šè­¦ï¼šCPUä½¿ç”¨ç‡90%")

# LangChain ä½¿ç”¨
response = langchain_llm.invoke("ç”Ÿæˆè¿ç»´æ£€æŸ¥æ¸…å•")
```

### æ™ºèƒ½ä½“é›†æˆ
```python
from src.agents.intelligent_ops_agent import IntelligentOpsAgent, AgentConfig

# åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆè‡ªåŠ¨ä½¿ç”¨ DeepSeekï¼‰
agent_config = AgentConfig(
    agent_id="deepseek_ops_agent",
    agent_type="general",
    specialization="deepseek_powered"
)

agent = IntelligentOpsAgent(agent_config)

# å¤„ç†å‘Šè­¦
alert = {
    "alert_id": "cpu_high_001",
    "severity": "high",
    "message": "CPUä½¿ç”¨ç‡æŒç»­è¶…è¿‡85%",
    "metrics": {"cpu_usage": 0.90}
}

result = await agent.process_alert(alert)
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿æ¥æµ‹è¯•
```bash
# æµ‹è¯•åŸºæœ¬è¿æ¥
python -c "
from src.utils.llm_config import test_llm_connection
test_llm_connection()
"
```

### åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œå®Œæ•´çš„ DeepSeek æµ‹è¯•å¥—ä»¶
python examples/deepseek_test.py
```

### æ€§èƒ½æµ‹è¯•
```python
import time
from src.utils.llm_config import setup_deepseek_llm, get_llm_config_from_env

# æµ‹è¯•å“åº”æ—¶é—´
config = get_llm_config_from_env()
dspy_lm, _ = setup_deepseek_llm(config)

start_time = time.time()
response = dspy_lm("æµ‹è¯•å“åº”æ—¶é—´")
end_time = time.time()

print(f"å“åº”æ—¶é—´: {end_time - start_time:.2f} ç§’")
```

## ğŸ”„ å¤šæ¨¡å‹åˆ‡æ¢

### é…ç½®å¤šä¸ªæä¾›å•†
```python
from src.utils.llm_config import LLMConfig, DEEPSEEK_CONFIGS, OPENAI_CONFIGS

# DeepSeek é…ç½®
deepseek_config = DEEPSEEK_CONFIGS["deepseek-chat"]
deepseek_config.api_key = "your-deepseek-key"

# OpenAI å¤‡ç”¨é…ç½®
openai_config = OPENAI_CONFIGS["gpt-4"]
openai_config.api_key = "your-openai-key"

# æ ¹æ®éœ€è¦åˆ‡æ¢
current_config = deepseek_config  # æˆ– openai_config
```

### ç¯å¢ƒå˜é‡åˆ‡æ¢
```bash
# åˆ‡æ¢åˆ° DeepSeek
export LLM_PROVIDER="deepseek"
export DEEPSEEK_API_KEY="your-key"

# åˆ‡æ¢åˆ° OpenAI
export LLM_PROVIDER="openai"
export OPENAI_API_KEY="your-key"

# åˆ‡æ¢åˆ°æœ¬åœ° Ollama
export LLM_PROVIDER="ollama"
export OLLAMA_BASE_URL="http://localhost:11434"
```

## ğŸ“Š æˆæœ¬ä¼˜åŒ–

### æˆæœ¬æ§åˆ¶ç­–ç•¥
```python
# è®¾ç½®åˆç†çš„ä»¤ç‰Œé™åˆ¶
config = LLMConfig(
    max_tokens=1000,        # é™åˆ¶è¾“å‡ºé•¿åº¦
    temperature=0.1,        # å‡å°‘é‡å¤å°è¯•
    timeout=30              # é¿å…é•¿æ—¶é—´ç­‰å¾…
)

# ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
import functools

@functools.lru_cache(maxsize=128)
def cached_llm_call(prompt: str) -> str:
    return dspy_lm(prompt)
```

### ç›‘æ§ä½¿ç”¨æƒ…å†µ
```python
class UsageTracker:
    def __init__(self):
        self.total_tokens = 0
        self.total_requests = 0
    
    def track_usage(self, response):
        if hasattr(response, 'usage'):
            self.total_tokens += response.usage.total_tokens
            self.total_requests += 1
    
    def get_cost_estimate(self):
        # DeepSeek ä»·æ ¼çº¦ä¸º $0.0014/1K tokens
        return self.total_tokens * 0.0014 / 1000

tracker = UsageTracker()
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. API Key é”™è¯¯
```
é”™è¯¯: DeepSeek API key is required
è§£å†³: è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡
```

#### 2. è¿æ¥è¶…æ—¶
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
config = LLMConfig(timeout=120)
```

#### 3. é€Ÿç‡é™åˆ¶
```python
import time
import random

def retry_with_backoff(func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if "rate limit" in str(e).lower():
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                time.sleep(wait_time)
            else:
                raise e
    raise Exception("Max retries exceeded")
```

#### 4. ä¸­æ–‡ç¼–ç é—®é¢˜
```python
# ç¡®ä¿æ­£ç¡®çš„ç¼–ç è®¾ç½®
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'
```

### æ—¥å¿—è°ƒè¯•
```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# åœ¨ LLM è°ƒç”¨ä¸­æ·»åŠ æ—¥å¿—
logger.debug(f"å‘é€è¯·æ±‚: {prompt}")
response = dspy_lm(prompt)
logger.debug(f"æ”¶åˆ°å“åº”: {response}")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†
```python
async def batch_process_alerts(alerts):
    tasks = []
    for alert in alerts:
        task = agent.process_alert(alert)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### å¹¶å‘æ§åˆ¶
```python
import asyncio

# é™åˆ¶å¹¶å‘æ•°é‡
semaphore = asyncio.Semaphore(5)

async def limited_llm_call(prompt):
    async with semaphore:
        return await llm_call(prompt)
```

## ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ

### API Key ç®¡ç†
```bash
# ä½¿ç”¨ç¯å¢ƒå˜é‡è€Œä¸æ˜¯ç¡¬ç¼–ç 
export DEEPSEEK_API_KEY="sk-..."

# æˆ–ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡
# AWS Secrets Manager, Azure Key Vault ç­‰
```

### æ•æ„Ÿæ•°æ®å¤„ç†
```python
import re

def sanitize_prompt(prompt: str) -> str:
    # ç§»é™¤æ•æ„Ÿä¿¡æ¯
    prompt = re.sub(r'\b\d{4}-\d{4}-\d{4}-\d{4}\b', '[CARD_MASKED]', prompt)
    prompt = re.sub(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b', '[IP_MASKED]', prompt)
    return prompt
```

## ğŸ“š å‚è€ƒèµ„æº

- [DeepSeek å®˜æ–¹æ–‡æ¡£](https://api-docs.deepseek.com/)
- [DSPy æ–‡æ¡£](https://dspy-docs.vercel.app/)
- [LangChain æ–‡æ¡£](https://python.langchain.com/docs/get_started/introduction)
- [æ™ºèƒ½è¿ç»´æœ€ä½³å®è·µ](./ops_best_practices.md)

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **ç”Ÿäº§éƒ¨ç½²**: é…ç½®è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
2. **ç›‘æ§å‘Šè­¦**: è®¾ç½® API ä½¿ç”¨é‡å’Œæˆæœ¬ç›‘æ§
3. **æ¨¡å‹å¾®è°ƒ**: åŸºäºè¿ç»´æ•°æ®ä¼˜åŒ–æ¨¡å‹è¡¨ç°
4. **å¤šæ¨¡æ€é›†æˆ**: ç»“åˆå›¾åƒè¯†åˆ«åˆ†æè¿ç»´å›¾è¡¨