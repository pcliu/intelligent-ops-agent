"""
LLM é…ç½®æ¨¡å—
æ”¯æŒ DeepSeekã€OpenAI ç­‰å¤šç§è¯­è¨€æ¨¡å‹
"""

import os
import dspy
from typing import Dict, Any, Optional
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI
from dataclasses import dataclass


@dataclass
class LLMConfig:
    """LLM é…ç½®ç±»"""
    provider: str = "deepseek"  # deepseek, openai, ollama
    model_name: str = "deepseek-chat"
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 2000
    timeout: int = 60


class DeepSeekLLM(dspy.LM):
    """DeepSeek LLM é€‚é…å™¨ï¼Œç”¨äº DSPy"""
    
    def __init__(self, 
                 model: str = "deepseek-chat",
                 api_key: Optional[str] = None,
                 base_url: str = "https://api.deepseek.com/v1",
                 **kwargs):
        
        self.model = model
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url
        self.kwargs = kwargs
        
        if not self.api_key:
            raise ValueError("DeepSeek API key is required. Set DEEPSEEK_API_KEY environment variable.")
        
        # åˆå§‹åŒ– OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯
        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
        except ImportError:
            raise ImportError("Please install openai package: uv add openai")
        
        super().__init__(model=model)
    
    def basic_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """åŸºç¡€è¯·æ±‚æ–¹æ³•"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=kwargs.get("temperature", 0.1),
                max_tokens=kwargs.get("max_tokens", 2000),
                **{k: v for k, v in kwargs.items() if k not in ["temperature", "max_tokens"]}
            )
            
            return {
                "choices": [{
                    "text": response.choices[0].message.content,
                    "finish_reason": response.choices[0].finish_reason
                }],
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
        except Exception as e:
            return {
                "choices": [{"text": f"Error: {str(e)}", "finish_reason": "error"}],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            }
    
    def __call__(self, prompt: str, **kwargs) -> str:
        """è°ƒç”¨æ–¹æ³•"""
        response = self.basic_request(prompt, **kwargs)
        return response["choices"][0]["text"]


def setup_deepseek_llm(config: Optional[LLMConfig] = None) -> tuple:
    """
    è®¾ç½® DeepSeek LLM ç”¨äº DSPy å’Œ LangChain
    
    Args:
        config: LLM é…ç½®
        
    Returns:
        tuple: (dspy_lm, langchain_llm)
    """
    if config is None:
        config = LLMConfig()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if not os.getenv("DEEPSEEK_API_KEY"):
        api_key = config.api_key or os.getenv("DEEPSEEK_API_KEY")
        if api_key:
            os.environ["DEEPSEEK_API_KEY"] = api_key
    
    # DSPy LLM é…ç½®
    if config.provider.lower() == "deepseek":
        dspy_lm = DeepSeekLLM(
            model=config.model_name,
            api_key=config.api_key,
            base_url=config.base_url or "https://api.deepseek.com/v1",
            temperature=config.temperature,
            max_tokens=config.max_tokens
        )
        
        # LangChain LLM é…ç½® (ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£)
        langchain_llm = ChatOpenAI(
            model=config.model_name,
            openai_api_key=config.api_key or os.getenv("DEEPSEEK_API_KEY"),
            openai_api_base=config.base_url or "https://api.deepseek.com/v1",
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            timeout=config.timeout
        )
    
    elif config.provider.lower() == "openai":
        # OpenAI é…ç½®
        dspy_lm = dspy.OpenAI(
            model=config.model_name,
            api_key=config.api_key,
            temperature=config.temperature,
            max_tokens=config.max_tokens
        )
        
        langchain_llm = ChatOpenAI(
            model=config.model_name,
            openai_api_key=config.api_key,
            temperature=config.temperature,
            max_tokens=config.max_tokens
        )
    
    elif config.provider.lower() == "ollama":
        # Ollama æœ¬åœ°é…ç½®
        dspy_lm = dspy.OllamaLocal(
            model=config.model_name,
            base_url=config.base_url or "http://localhost:11434"
        )
        
        langchain_llm = Ollama(
            model=config.model_name,
            base_url=config.base_url or "http://localhost:11434"
        )
    
    else:
        raise ValueError(f"Unsupported LLM provider: {config.provider}")
    
    # è®¾ç½® DSPy é»˜è®¤ LM
    dspy.settings.configure(lm=dspy_lm)
    
    return dspy_lm, langchain_llm


def get_llm_config_from_env() -> LLMConfig:
    """ä»ç¯å¢ƒå˜é‡è·å– LLM é…ç½®"""
    return LLMConfig(
        provider=os.getenv("LLM_PROVIDER", "deepseek"),
        model_name=os.getenv("LLM_MODEL_NAME", "deepseek-chat"),
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url=os.getenv("LLM_BASE_URL", "https://api.deepseek.com/v1"),
        temperature=float(os.getenv("LLM_TEMPERATURE", "0.1")),
        max_tokens=int(os.getenv("LLM_MAX_TOKENS", "2000")),
        timeout=int(os.getenv("LLM_TIMEOUT", "60"))
    )


def validate_llm_config(config: LLMConfig) -> bool:
    """éªŒè¯ LLM é…ç½®"""
    if config.provider.lower() == "deepseek":
        if not config.api_key and not os.getenv("DEEPSEEK_API_KEY"):
            print("âŒ DeepSeek API key is required")
            return False
    elif config.provider.lower() == "openai":
        if not config.api_key and not os.getenv("OPENAI_API_KEY"):
            print("âŒ OpenAI API key is required")
            return False
    
    if config.temperature < 0 or config.temperature > 2:
        print("âŒ Temperature should be between 0 and 2")
        return False
    
    if config.max_tokens <= 0:
        print("âŒ Max tokens should be positive")
        return False
    
    return True


def test_llm_connection(config: Optional[LLMConfig] = None) -> bool:
    """æµ‹è¯• LLM è¿æ¥"""
    try:
        if config is None:
            config = get_llm_config_from_env()
        
        print(f"ğŸ”„ Testing {config.provider} connection...")
        
        dspy_lm, langchain_llm = setup_deepseek_llm(config)
        
        # æµ‹è¯• DSPy
        test_prompt = "Hello, please respond with 'Connection successful'"
        response = dspy_lm(test_prompt)
        
        print(f"âœ… {config.provider} connection successful")
        print(f"   Model: {config.model_name}")
        print(f"   Response: {response[:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ {config.provider if config else 'LLM'} connection failed: {str(e)}")
        return False


# é¢„è®¾é…ç½®
DEEPSEEK_CONFIGS = {
    "deepseek-chat": LLMConfig(
        provider="deepseek",
        model_name="deepseek-chat",
        base_url="https://api.deepseek.com/v1",
        temperature=0.1,
        max_tokens=2000
    ),
    "deepseek-coder": LLMConfig(
        provider="deepseek",
        model_name="deepseek-coder",
        base_url="https://api.deepseek.com/v1",
        temperature=0.1,
        max_tokens=4000
    )
}

OPENAI_CONFIGS = {
    "gpt-4": LLMConfig(
        provider="openai",
        model_name="gpt-4",
        temperature=0.1,
        max_tokens=2000
    ),
    "gpt-3.5-turbo": LLMConfig(
        provider="openai",
        model_name="gpt-3.5-turbo",
        temperature=0.1,
        max_tokens=2000
    )
}